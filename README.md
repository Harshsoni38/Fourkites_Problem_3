# ğŸ” Loss Landscape Geometry & Optimization Dynamics  
### *A Practical Framework Connecting Curvature, SGD Noise, Generalization, and Architecture Design*

This repository implements a rigorous framework for analyzing neural network **loss landscapes**, including curvature (Hessian spectrum), sharpness, and 2D loss geometry, and relates these properties to SGD optimization dynamics and generalization. Experiments are performed on MNIST across two architectures (MLP vs CNN) and two optimization regimes (high-noise vs low-noise SGD).

The framework is modular, reproducible, and easily extendable to **OCR models**, **LLMs**, **Transformers**, and large-scale deep networks.

---

## ğŸš€ Highlights

- âš¡ Efficient **Hessianâ€“vector product** curvature estimation  
- ğŸ“ˆ **Power iteration** to compute largest eigenvalue (Î»_max)  
- ğŸ” **Random-direction sharpness** computation  
- ğŸŒ„ **2D loss landscape slicing & visualization**  
- ğŸ§± Architecture comparison: **MLP vs CNN**  
- ğŸ”¥ High-noise vs low-noise SGD generalization study  
- ğŸ“Š Clean PyTorch experiments with CSV logging  
- ğŸ“„ Complete LaTeX research report  

---

## ğŸ“ Repository Structure

â”œâ”€â”€ run_experiments.py # Train models, log loss/accuracy/Î»_max/sharpness
â”œâ”€â”€ compare_landscape_results.py # Generate comparison plots (loss, accuracy, Î»_max, sharpness, gap)
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ logs_experiments/ # Auto-created logs for each experiment
â”œâ”€â”€ plots/ # All generated plots
â”‚ â”œâ”€â”€ loss_curves.png
â”‚ â”œâ”€â”€ accuracy_curves.png
â”‚ â”œâ”€â”€ lambda_max.png
â”‚ â”œâ”€â”€ sharpness.png
â”‚ â””â”€â”€ gen_gap_vs_lambda.png
â””â”€â”€ latex/ # LaTeX report


---

## ğŸ“¦ Installation

git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
pip install -r requirements.txt
torch
torchvision
numpy
matplotlib
tqdm


---

## ğŸ§  Conceptual Overview

This project connects three major aspects of deep learning:

### 1. Loss Landscape Geometry
- Hessian spectrum  
- Largest eigenvalue Î»_max  
- Sharpness under random perturbations  
- 1D/2D loss slices  
- Effective dimensionality of curvature  

### 2. Stochastic Optimization Dynamics
- SGD as stochastic differential equation  
- Gradient noise acts as â€œtemperatureâ€  
- High-noise â†’ flatter minima  
- Low-noise â†’ sharp minima  

### 3. Generalization Behavior
- Generalization gap (train vs test)  
- Strong correlation between Î»_max and generalization  
- Flat minima consistently generalize better  

---

## ğŸ§ª Running Experiments

### Train all four experiments (MLP/CNN Ã— high/low noise)


Logs saved under:


Logged metrics:

- epoch  
- train/val/test loss  
- train/val/test accuracy  
- Î»_max (largest Hessian eigenvalue)  
- sharp_mean / sharp_max  

Also stores:

loss_slice_2d.pt


Generated output:

- loss_curves.png  
- accuracy_curves.png  
- lambda_max.png  
- sharpness.png  
- gen_gap_vs_lambda.png  

---

## ğŸ“‰ Summary of Experimental Findings

### âœ” Loss Curves
CNNs converge faster and achieve lower loss.  
High-noise SGD smooths early optimization.

### âœ” Accuracy Curves
CNN reaches >99% rapidly.  
High-noise improves early generalization.

### âœ” Curvature (Î»_max)
- CNN low-noise â†’ Î»_max > 100 (very sharp)  
- CNN high-noise â†’ Î»_max â‰ˆ 5â€“15 (flat)  
- MLPs follow same trend at smaller scale  

### âœ” Sharpness
Matches Î»_max trends across all runs.

### âœ” Generalization vs Curvature
Flatter minima â†’ smaller generalization gap  
Sharp minima â†’ worse generalization  
Matches results from Keskar et al. (2017).  

---

## ğŸ”® Extensions to LLMs & OCR Systems

### LLM Training / Fine-tuning:
- Track curvature spikes for instability detection  
- Identify sharp layers (attention blocks)  
- Guide LR warmup/decay, clipping, and LoRA tuning  
- High-noise fine-tuning improves generalization  

### OCR Models:
- CNN/Vision Transformer encoders show different curvature profiles  
- Sharp minima = overfitting to noisy scans  
- Use curvature to choose robust model/encoder  

---

## ğŸ“„Research Report

The `latex/` folder includes a full scientific paper template with:
- Theory  
- Methods  
- Results  
- Figures  
- Interpretation  
- Application to LLM/OCR  

Ready to compile in Overleaf.

---

## ğŸ¤ Contributing

Possible extensions:

- Add ResNet/Vision Transformer experiments  
- Layer-wise curvature tracking  
- Fisher information-based curvature  
- Optimization trajectory visualization  

